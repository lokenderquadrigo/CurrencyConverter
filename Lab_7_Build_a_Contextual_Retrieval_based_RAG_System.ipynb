{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokenderquadrigo/CurrencyConverter/blob/main/Lab_7_Build_a_Contextual_Retrieval_based_RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Contextual Retrieval based RAG System"
      ],
      "metadata": {
        "id": "jbw4wHV4zlKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "4vtFl39Ofu_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain #==0.3.10\n",
        "!pip install langchain-openai #==0.2.12\n",
        "!pip install langchain-community #==0.3.11"
      ],
      "metadata": {
        "id": "LVX6450Lfu_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install jq #==1.8.0"
      ],
      "metadata": {
        "id": "YikT3_10w1PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pymupdf #==1.25.1"
      ],
      "metadata": {
        "id": "fUdOiWH6w4GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB and LangChain wrapper"
      ],
      "metadata": {
        "id": "bwUBYHjPfu_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain-chroma #==0.1.4"
      ],
      "metadata": {
        "id": "p30SmCgTfu__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "EITC17hwfu__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "yEh2olNvfvAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "jiokYxD8fvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "-On4AS0HfvAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "1571dffa-2d6f-48a3-a64d-26ae0add5665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-359396342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# details here: https://openai.com/blog/new-embedding-models-and-api-updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopenai_embed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-embedding-3-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Module for OpenAI integrations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureOpenAIEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Module for OpenAI chat models.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mazure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/azure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguageModelInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLangSmithParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Data"
      ],
      "metadata": {
        "id": "afzeN_WkHIz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n",
        "# manually upload it on colab\n",
        "!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rag_docs.zip"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process JSON Documents"
      ],
      "metadata": {
        "id": "wMlxKZ_5jIdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n",
        "                    jq_schema='.',\n",
        "                    text_content=False,\n",
        "                    json_lines=True)\n",
        "wiki_docs = loader.load()"
      ],
      "metadata": {
        "id": "RZ5y0NfzHPhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs[3]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "wiki_docs_processed = []\n",
        "\n",
        "for doc in wiki_docs:\n",
        "    doc = json.loads(doc.page_content)\n",
        "    metadata = {\n",
        "        \"title\": doc['title'],\n",
        "        \"id\": doc['id'],\n",
        "        \"source\": \"Wikipedia\",\n",
        "        \"page\": 1\n",
        "    }\n",
        "    data = ' '.join(doc['paragraphs'])\n",
        "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
      ],
      "metadata": {
        "id": "yICyAF85h2DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs_processed[3]"
      ],
      "metadata": {
        "id": "6IATrHWKh7II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process PDF documents"
      ],
      "metadata": {
        "id": "F_GzvHP1jSBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Chunk Contexts for Contextual Retrieval\n",
        "\n",
        "![](https://i.imgur.com/LRhKHzk.png)"
      ],
      "metadata": {
        "id": "4vH6xGFOnv7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "jxHHyhlbl_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from the LangChain framework and standard libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# --- Helper Function to Generate Context for a Single Chunk ---\n",
        "\n",
        "def generate_chunk_context(document, chunk):\n",
        "    \"\"\"\n",
        "    Generates a brief, contextual summary for a text chunk based on the entire document.\n",
        "\n",
        "    Args:\n",
        "        document (str): The full content of the original document.\n",
        "        chunk (str): The specific text chunk that needs a contextual summary.\n",
        "\n",
        "    Returns:\n",
        "        str: An AI-generated contextual summary for the chunk.\n",
        "    \"\"\"\n",
        "\n",
        "    # This is the prompt template that instructs the language model.\n",
        "    # It clearly defines the role of the AI, the inputs (the full paper and the specific chunk),\n",
        "    # and the desired output format (a concise context).\n",
        "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
        "                            Your task is to provide brief, relevant context for a chunk of text\n",
        "                            based on the following research paper.\n",
        "\n",
        "                            Here is the research paper:\n",
        "                            <paper>\n",
        "                            {paper}\n",
        "                            </paper>\n",
        "\n",
        "                            Here is the chunk we want to situate within the whole document:\n",
        "                            <chunk>\n",
        "                            {chunk}\n",
        "                            </chunk>\n",
        "\n",
        "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
        "                            considering the following guidelines:\n",
        "\n",
        "                            - Give a short succinct context to situate this chunk within the overall document\n",
        "                            for the purposes of improving search retrieval of the chunk.\n",
        "                            - Answer only with the succinct context and nothing else.\n",
        "                            - Context should be mentioned like 'Focuses on ....'\n",
        "                            do not mention 'this chunk or section focuses on...'\n",
        "\n",
        "                            Context:\n",
        "                         \"\"\"\n",
        "\n",
        "    # Create a prompt object from the string template\n",
        "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
        "\n",
        "    # Define the \"chain\" using LangChain Expression Language (LCEL).\n",
        "    # 1. The prompt_template takes the 'paper' and 'chunk' inputs.\n",
        "    # 2. The formatted prompt is passed to the language model ('chatgpt').\n",
        "    # 3. The model's output is parsed into a simple string by 'StrOutputParser'.\n",
        "    agentic_chunk_chain = (prompt_template\n",
        "                           |\n",
        "                           chatgpt # Your language model instance\n",
        "                           |\n",
        "                           StrOutputParser())\n",
        "\n",
        "    # Execute the chain by providing the actual document and chunk content.\n",
        "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
        "\n",
        "    # Return the generated context string.\n",
        "    return context"
      ],
      "metadata": {
        "id": "mM5akEgv0lHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid # Used for generating unique IDs for each chunk\n",
        "\n",
        "# --- Main Function to Create Contextual Chunks from a File ---\n",
        "\n",
        "def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
        "    \"\"\"\n",
        "    Loads a PDF, splits it into chunks, and adds a contextual summary to each chunk.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the PDF file.\n",
        "        chunk_size (int): The maximum number of characters for each chunk.\n",
        "        chunk_overlap (int): The number of characters to overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: A list of LangChain Document objects, where each document's\n",
        "                        page_content is the context + original chunk text.\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading pages:', file_path)\n",
        "    # Initialize the PDF loader with the file path\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    # Load the document into a list of LangChain Document objects, one per page\n",
        "    doc_pages = loader.load()\n",
        "\n",
        "    print('Chunking pages:', file_path)\n",
        "    # Initialize the text splitter with the desired chunk size and overlap\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                              chunk_overlap=chunk_overlap)\n",
        "    # Split the loaded pages into smaller chunks\n",
        "    doc_chunks = splitter.split_documents(doc_pages)\n",
        "\n",
        "    print('Generating contextual chunks:', file_path)\n",
        "    # Reassemble the entire document's text to provide full context for each chunk\n",
        "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
        "\n",
        "    # This list will store the final, enriched chunks\n",
        "    contextual_chunks = []\n",
        "\n",
        "    # Loop through each raw chunk to generate its context\n",
        "    for chunk in doc_chunks:\n",
        "        # Get the text content and metadata from the current chunk\n",
        "        chunk_content = chunk.page_content\n",
        "        chunk_metadata = chunk.metadata\n",
        "\n",
        "        # Create a new, updated metadata dictionary for better organization\n",
        "        chunk_metadata_upd = {\n",
        "            'id': str(uuid.uuid4()),  # Add a unique identifier\n",
        "            'page': chunk_metadata['page'],  # Keep the original page number\n",
        "            'source': chunk_metadata['source'],  # Keep the original file path\n",
        "            'title': chunk_metadata['source'].split('/')[-1] # Extract just the filename as title\n",
        "        }\n",
        "\n",
        "        # Call our helper function to generate the context for this specific chunk\n",
        "        context = generate_chunk_context(original_doc, chunk_content)\n",
        "\n",
        "        # Create a new LangChain Document object\n",
        "        # The content is the generated context, a newline, and then the original chunk's content\n",
        "        # The metadata is our newly created dictionary\n",
        "        contextual_chunks.append(Document(page_content=context + '\\n' + chunk_content,\n",
        "                                          metadata=chunk_metadata_upd))\n",
        "\n",
        "    print('Finished processing:', file_path)\n",
        "    print()\n",
        "\n",
        "    # Return the list of newly created, context-aware chunks\n",
        "    return contextual_chunks"
      ],
      "metadata": {
        "id": "-uxOSfcsxqHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "pdf_files = glob('./rag_docs/*.pdf')\n",
        "pdf_files"
      ],
      "metadata": {
        "id": "i-VUpdmczt0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs = []\n",
        "for fp in pdf_files:\n",
        "    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"
      ],
      "metadata": {
        "id": "EsicBMPazzlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_docs)"
      ],
      "metadata": {
        "id": "oOkwPMxp0gFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs[0]"
      ],
      "metadata": {
        "id": "qVK2i0lR0ieV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(paper_docs[10].page_content))"
      ],
      "metadata": {
        "id": "rCtsR5bYzydp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine all document chunks in one list"
      ],
      "metadata": {
        "id": "UyPdlZo2xEly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs_processed)"
      ],
      "metadata": {
        "id": "UbtpR-r50mEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = wiki_docs_processed + paper_docs\n",
        "len(total_docs)"
      ],
      "metadata": {
        "id": "lNQWgq9t0pMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Document Chunks and Embeddings in Vector DB\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "Daqn6Hglw9Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=total_docs,\n",
        "                                  collection_name='my_context_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./my_context_db\")"
      ],
      "metadata": {
        "id": "ZhAQyrFBfvAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "### Load Vector DB from disk\n",
        "\n",
        "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./my_context_db\",\n",
        "                   collection_name='my_context_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC3uPqYop0a"
      },
      "outputs": [],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Similarity based Retrieval\n",
        "\n",
        "We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"
      ],
      "metadata": {
        "id": "njfZOOVZxj1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "tV1l6HYdxj1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_docs(docs):\n",
        "    for doc in docs:\n",
        "        print('Metadata:', doc.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(doc.page_content[:1000]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "nUIJG_bDxj1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is machine learning?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "PIh4xGv2xj1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the difference between transformers and vision transformers?\"\n",
        "top_docs = similarity_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "id": "S_PXFMcJxuyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the RAG Pipeline"
      ],
      "metadata": {
        "id": "gQFWv7YUyVII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
      ],
      "metadata": {
        "id": "PHOrfGXKyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (similarity_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    rag_prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "KmWeCB4yyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "query = \"What is machine learning?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "xvj_eGIWyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "pXtezDlZzadt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is a resnet better than a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "Fo92-ZmIELPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is NLP and its relation to linguistics?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "J5IQoBc0zlAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI, ML and DL?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "AzeZuG1hzvGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "cWihDiL3zPzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is self-attention important in transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "k1lqzejlEvsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does a resnet work?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "7Zf_BjmlFBcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangGraph?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "UbaUpVXKz8IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is an Agentic AI System?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "FmxNo6B50AdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangChain?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "Ttz53mEy0J_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a RAG System with Sources"
      ],
      "metadata": {
        "id": "-HkUAnWFH8LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
      ],
      "metadata": {
        "id": "cd9CWH6hH8LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary components from LangChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from operator import itemgetter # A standard Python function to get an item from a dictionary\n",
        "\n",
        "# --- Initialization ---\n",
        "# Initialize the language model instance. We'll use this for generation.\n",
        "# \"gpt-4o-mini\" is a fast and capable model. temperature=0 makes its output more deterministic.\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# NOTE: The following variables are assumed to be defined elsewhere in your code:\n",
        "# - similarity_retriever: A LangChain retriever object that fetches documents based on a query.\n",
        "# - rag_prompt_template: A ChatPromptTemplate that takes \"context\" and \"question\" as input.\n",
        "\n",
        "\n",
        "# --- Helper Function ---\n",
        "def format_docs(docs):\n",
        "    \"\"\"A helper function to concatenate the page_content of multiple documents into a single string.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# --- Chain 1: The Core RAG Generation Chain ---\n",
        "# This chain is responsible for generating the final answer once it has the context and question.\n",
        "src_rag_response_chain = (\n",
        "    # This dictionary defines the inputs for the prompt template.\n",
        "    {\n",
        "        # The \"context\" key's value is processed in two steps:\n",
        "        # 1. itemgetter('context'): Extracts the list of documents from the input dictionary.\n",
        "        # 2. RunnableLambda(format_docs): Applies our helper function to format the documents into a single string.\n",
        "        \"context\": (itemgetter('context') | RunnableLambda(format_docs)),\n",
        "\n",
        "        # The \"question\" key's value is simply extracted from the input dictionary.\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    |\n",
        "    # The dictionary with formatted context and question is passed to the prompt template.\n",
        "    rag_prompt_template\n",
        "    |\n",
        "    # The formatted prompt is passed to the language model.\n",
        "    chatgpt\n",
        "    |\n",
        "    # The output from the model is parsed into a clean string.\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 2: The Full RAG Pipeline that includes Sources ---\n",
        "# This is the main chain that orchestrates the entire process, from question to final output.\n",
        "rag_chain_w_sources = (\n",
        "    # This initial step performs retrieval. The input to this whole chain is the user's question.\n",
        "    {\n",
        "        # The retriever is called with the user's question to fetch relevant documents.\n",
        "        # The result is assigned to the \"context\" key.\n",
        "        \"context\": similarity_retriever,\n",
        "\n",
        "        # RunnablePassthrough() simply passes the original user question through to the \"question\" key.\n",
        "        # The output of this dictionary is: {\"context\": [docs], \"question\": \"user's question\"}\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    |\n",
        "    # RunnablePassthrough.assign() is a key component. It takes the dictionary from the previous step\n",
        "    # and adds a new key to it. Here, we are adding a key named \"response\".\n",
        "    # The value of \"response\" is generated by running the `src_rag_response_chain` with the\n",
        "    # context and question from the previous step.\n",
        "    RunnablePassthrough.assign(response=src_rag_response_chain)\n",
        ")\n",
        "\n",
        "# Final Output Structure when invoking `rag_chain_w_sources`:\n",
        "# {\n",
        "#   'context': [Document(page_content='...'), ...],  <- The source documents\n",
        "#   'question': 'What is the capital of France?',     <- The original question\n",
        "#   'response': 'The capital of France is Paris.'     <- The LLM's answer\n",
        "# }"
      ],
      "metadata": {
        "id": "m6wFtKQqXNHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "fYfU-VdjYf1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_results(result_obj):\n",
        "    print('Query:')\n",
        "    display(Markdown(result_obj['question']))\n",
        "    print()\n",
        "    print('Response:')\n",
        "    display(Markdown(result_obj['response']))\n",
        "    print('='*50)\n",
        "    print('Sources:')\n",
        "    for source in result_obj['context']:\n",
        "        print('Metadata:', source.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(source.page_content))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "2wWdkU4oa1BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "8NTfGcChbFuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI, ML and DL?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "jhWbcmodbYfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "iKggz7qfYkh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is an Agentic AI System?\"\n",
        "result = rag_chain_w_sources.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "fCCvcwzEbqQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a RAG System with Source Citations Agentic Pipeline"
      ],
      "metadata": {
        "id": "25Si_mSAc9HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
        "rag_prompt_template.pretty_print()"
      ],
      "metadata": {
        "id": "x3cKwTUIc9HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citations_prompt = \"\"\"You are an assistant who is an expert in analyzing answers to questions\n",
        "                      and finding out referenced citations from context articles.\n",
        "\n",
        "                      Given the following question, context and generated answer,\n",
        "                      analyze the generated answer and quote citations from context articles\n",
        "                      that can be used to justify the generated answer.\n",
        "\n",
        "                      Question:\n",
        "                      {question}\n",
        "\n",
        "                      Context Articles:\n",
        "                      {context}\n",
        "\n",
        "                      Answer:\n",
        "                      {answer}\n",
        "                  \"\"\"\n",
        "\n",
        "cite_prompt_template = ChatPromptTemplate.from_template(citations_prompt)\n",
        "cite_prompt_template.pretty_print()"
      ],
      "metadata": {
        "id": "0fbhOllRprRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class Citation(BaseModel):\n",
        "    id: str = Field(description=\"\"\"The string ID of a SPECIFIC context article\n",
        "                                   which justifies the answer.\"\"\")\n",
        "    source: str = Field(description=\"\"\"The source of the SPECIFIC context article\n",
        "                                       which justifies the answer.\"\"\")\n",
        "    title: str = Field(description=\"\"\"The title of the SPECIFIC context article\n",
        "                                      which justifies the answer.\"\"\")\n",
        "    page: int = Field(description=\"\"\"The page number of the SPECIFIC context article\n",
        "                                     which justifies the answer.\"\"\")\n",
        "    quotes: str = Field(description=\"\"\"The VERBATIM sentences from the SPECIFIC context article\n",
        "                                      that are used to generate the answer.\n",
        "                                      Should be exact sentences from context article without missing words.\"\"\")\n",
        "\n",
        "\n",
        "class QuotedCitations(BaseModel):\n",
        "    \"\"\"Quote citations from given context articles\n",
        "       that can be used to justify the generated answer. Can be multiple articles.\"\"\"\n",
        "    citations: List[Citation] = Field(description=\"\"\"Citations (can be multiple) from the given\n",
        "                                                     context articles that justify the answer.\"\"\")"
      ],
      "metadata": {
        "id": "n-ehwnM4dyGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "\n",
        "# Standard language model for generating plain text answers.\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# A specialized version of the model that is forced to output JSON matching the 'QuotedCitations' schema.\n",
        "# This is the key to getting reliable, structured citation data.\n",
        "structured_chatgpt = chatgpt.with_structured_output(QuotedCitations)\n",
        "\n",
        "# NOTE: These variables are assumed to be defined elsewhere:\n",
        "# - similarity_retriever: A retriever that fetches relevant documents.\n",
        "# - rag_prompt_template: A prompt for generating the initial answer.\n",
        "# - cite_prompt_template: A prompt specifically for generating citations based on an answer.\n",
        "\n",
        "\n",
        "# --- Helper Function ---\n",
        "\n",
        "def format_docs_with_metadata(docs: List[Document]) -> str:\n",
        "    \"\"\"\n",
        "    Formats documents to include their metadata explicitly in the context string.\n",
        "    This helps the LLM to easily access information needed for citations.\n",
        "    \"\"\"\n",
        "    formatted_docs = [\n",
        "        f\"\"\"Context Article ID: {doc.metadata['id']}\n",
        "            Context Article Source: {doc.metadata['source']}\n",
        "            Context Article Title: {doc.metadata['title']}\n",
        "            Context Article Page: {doc.metadata['page']}\n",
        "            Context Article Details: {doc.page_content}\n",
        "         \"\"\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    ]\n",
        "    # Join all formatted doc strings into a single block of text.\n",
        "    return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "\n",
        "# --- Chain 1: Generates the initial text answer ---\n",
        "\n",
        "rag_response_chain = (\n",
        "    {\n",
        "        # Format the retrieved documents using our detailed helper function.\n",
        "        \"context\": (itemgetter('context') | RunnableLambda(format_docs_with_metadata)),\n",
        "        # Pass the original question through.\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    |\n",
        "    # Use the standard RAG prompt.\n",
        "    rag_prompt_template\n",
        "    |\n",
        "    # Use the standard chat model to generate a text answer.\n",
        "    chatgpt\n",
        "    |\n",
        "    # Parse the output into a string.\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 2: Generates structured citations for the answer ---\n",
        "\n",
        "cite_response_chain = (\n",
        "    {\n",
        "        # Pass the original, unformatted context through.\n",
        "        \"context\": itemgetter('context'),\n",
        "        # Pass the original question through.\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        # IMPORTANT: This chain requires the 'answer' generated by the previous chain.\n",
        "        \"answer\": itemgetter(\"answer\")\n",
        "    }\n",
        "    |\n",
        "    # Use the specialized citation prompt.\n",
        "    cite_prompt_template\n",
        "    |\n",
        "    # Use the structured output model to get a JSON object of citations.\n",
        "    structured_chatgpt\n",
        ")\n",
        "\n",
        "\n",
        "# --- Chain 3: The final orchestrator chain ---\n",
        "\n",
        "rag_chain_w_citations = (\n",
        "    # Step 1: Retrieval. Fetch context documents based on the question.\n",
        "    # Output of this step: {\"context\": [docs], \"question\": \"user_question\"}\n",
        "    {\n",
        "        \"context\": similarity_retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    |\n",
        "    # Step 2: Generate Answer. Run the first chain to get the text answer and\n",
        "    # add it to the dictionary under the key \"answer\".\n",
        "    # Output of this step: {\"context\": [docs], \"question\": \"user_question\", \"answer\": \"text_answer\"}\n",
        "    RunnablePassthrough.assign(answer=rag_response_chain)\n",
        "    |\n",
        "    # Step 3: Generate Citations. Run the second chain using the output from the previous step.\n",
        "    # The result (structured citations) is added to the dictionary under the key \"citations\".\n",
        "    # Output of this step: {\"context\": ..., \"question\": ..., \"answer\": ..., \"citations\": ...}\n",
        "    RunnablePassthrough.assign(citations=cite_response_chain)\n",
        ")"
      ],
      "metadata": {
        "id": "qt0t3-8T4Doi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is machine learning\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "Qt12nMvTjVp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['citations'].dict()['citations']"
      ],
      "metadata": {
        "id": "L3fufsoAHiuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# used mostly for nice display formatting, ignore if not needed\n",
        "def get_cited_context(result_obj):\n",
        "    # Dictionary to hold separate citation information for each unique source and title combination\n",
        "    source_with_citations = {}\n",
        "\n",
        "    def highlight_text(context, quote):\n",
        "        # Normalize whitespace and remove unnecessary punctuation\n",
        "        quote = re.sub(r'\\s+', ' ', quote).strip()\n",
        "        context = re.sub(r'\\s+', ' ', context).strip()\n",
        "\n",
        "        # Split quote into phrases, being careful with punctuation\n",
        "        phrases = [phrase.strip() for phrase in re.split(r'[.!?]', quote) if phrase.strip()]\n",
        "\n",
        "        highlighted_context = context\n",
        "\n",
        "        for phrase in phrases: # for each quoted phrase\n",
        "\n",
        "            # Create regex pattern to match cited phrases\n",
        "            # Escape special regex characters, but preserve word boundaries\n",
        "            escaped_phrase = re.escape(phrase)\n",
        "            # Create regex pattern that allows for slight variations\n",
        "            pattern = re.compile(r'\\b' + escaped_phrase + r'\\b', re.IGNORECASE)\n",
        "\n",
        "            # Replace all matched phrases with bolded version\n",
        "            highlighted_context = pattern.sub(lambda m: f\"**{m.group(0)}**\", highlighted_context)\n",
        "\n",
        "        return highlighted_context\n",
        "\n",
        "    # Process the citation data\n",
        "    for cite in result_obj['citations'].dict()['citations']:\n",
        "        cite_id = cite['id']\n",
        "        title = cite['title']\n",
        "        source = cite['source']\n",
        "        page = cite['page']\n",
        "        quote = cite['quotes']\n",
        "\n",
        "        # Check if the (source, title) key exists, and initialize if it doesn't\n",
        "        if (source, title) not in source_with_citations:\n",
        "            source_with_citations[(source, title)] = {\n",
        "                'title': title,\n",
        "                'source': source,\n",
        "                'citations': []\n",
        "            }\n",
        "\n",
        "        # Find or create the citation entry for this unique (id, page) combination\n",
        "        citation_entry = next(\n",
        "            (c for c in source_with_citations[(source, title)]['citations'] if c['id'] == cite_id and c['page'] == page),\n",
        "            None\n",
        "        )\n",
        "        if citation_entry is None:\n",
        "            citation_entry = {'id': cite_id, 'page': page, 'quote': [quote], 'context': None}\n",
        "            source_with_citations[(source, title)]['citations'].append(citation_entry)\n",
        "        else:\n",
        "            citation_entry['quote'].append(quote)\n",
        "\n",
        "    # Process context data\n",
        "    for context in result_obj['context']:\n",
        "        context_id = context.metadata['id']\n",
        "        context_page = context.metadata['page']\n",
        "        source = context.metadata['source']\n",
        "        title = context.metadata['title']\n",
        "        page_content = context.page_content\n",
        "\n",
        "        # Match the context to the correct citation entry by source, title, id, and page\n",
        "        if (source, title) in source_with_citations:\n",
        "            for citation in source_with_citations[(source, title)]['citations']:\n",
        "                if citation['id'] == context_id and citation['page'] == context_page:\n",
        "                    # Apply highlighting for each quote in the citation's quote list\n",
        "                    highlighted_content = page_content\n",
        "                    for quote in citation['quote']:\n",
        "                        highlighted_content = highlight_text(highlighted_content, quote)\n",
        "                    citation['context'] = highlighted_content\n",
        "\n",
        "    # Convert the dictionary to a list of dictionaries for separate entries\n",
        "    final_result_list = [\n",
        "        {\n",
        "            'title': details['title'],\n",
        "            'source': details['source'],\n",
        "            'citations': details['citations']\n",
        "        }\n",
        "        for details in source_with_citations.values()\n",
        "    ]\n",
        "\n",
        "    return final_result_list\n"
      ],
      "metadata": {
        "id": "docbPBPDxSVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_cited_context(result)"
      ],
      "metadata": {
        "id": "Kq8XyKnlJb4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_results(result_obj):\n",
        "    print('Query:')\n",
        "    display(Markdown(result_obj['question']))\n",
        "    print()\n",
        "    print('Response:')\n",
        "    display(Markdown(result_obj['answer']))\n",
        "    print('='*50)\n",
        "    print('Sources:')\n",
        "    cited_context = get_cited_context(result_obj)\n",
        "    for source in cited_context:\n",
        "        print('Title:', source['title'], ' ', 'Source:', source['source'])\n",
        "        print('Citations:')\n",
        "        for citation in source['citations']:\n",
        "            print('ID:', citation['id'], ' ', 'Page:', citation['page'])\n",
        "            print('Cited Quotes:')\n",
        "            display(Markdown('*'+' '.join(citation['quote'])+'*'))\n",
        "            print('Cited Context:')\n",
        "            display(Markdown(citation['context']))\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "aChUXboG903B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_results(result)"
      ],
      "metadata": {
        "id": "pG6fcxAE3I3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is AI, ML and DL?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "rgTlW5hg_d0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is Machine learning related to supervised learning and clustering?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "Zho3RfTQ-_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = rag_chain_w_citations.invoke(query)\n",
        "display_results(result)"
      ],
      "metadata": {
        "id": "vy2zTEfAFxBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoTGhQSA3pod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}